import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import os

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
dataset_df = pd.read_csv("./dataset_bert.csv", sep=",", header=None)

# åˆå§‹åŒ– LabelEncoder
lbl = LabelEncoder()
labels = lbl.fit_transform(dataset_df[1].values[:500])
texts = list(dataset_df[0].values[:500])

# åˆ†å‰²æ•°æ®
x_train, x_test, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, stratify=labels
)

# åŠ è½½åˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('../../../../models/google-bert/bert-base-chinese')


# ç¼–ç æ•°æ®
def encode_texts(texts, labels=None):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=64,
        return_tensors='pt'
    )

    if labels is not None:
        return TensorDataset(
            encodings['input_ids'],
            encodings['attention_mask'],
            torch.tensor(labels, dtype=torch.long)
        )
    else:
        return encodings['input_ids'], encodings['attention_mask']


# åˆ›å»ºæ•°æ®é›†
train_dataset = encode_texts(x_train, train_labels)
test_dataset = encode_texts(x_test, test_labels)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# åŠ è½½æ¨¡å‹
model = BertForSequenceClassification.from_pretrained(
    '../../../../models/google-bert/bert-base-chinese',
    num_labels=17
)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# ä¼˜åŒ–å™¨
optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)


# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, optimizer, epoch):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    progress_bar = tqdm(train_loader, desc=f'è®­ç»ƒ Epoch {epoch + 1}')
    for batch in progress_bar:
        input_ids, attention_mask, labels = batch
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        labels = labels.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels)
        loss = outputs.loss

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        predictions = torch.argmax(outputs.logits, dim=1)
        correct += (predictions == labels).sum().item()
        total += labels.size(0)

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'loss': loss.item(),
            'acc': correct / total
        })

    avg_loss = total_loss / len(train_loader)
    accuracy = correct / total
    return avg_loss, accuracy


# è¯„ä¼°å‡½æ•°
def evaluate_model(model, test_loader):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids, attention_mask, labels = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            labels = labels.to(device)

            outputs = model(input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels)

            total_loss += outputs.loss.item()
            predictions = torch.argmax(outputs.logits, dim=1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)

            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy, all_predictions, all_labels


# é¢„æµ‹å‡½æ•°
def predict_texts(model, texts):
    """é¢„æµ‹è¾“å…¥æ–‡æœ¬çš„åˆ†ç±»"""
    model.eval()

    # ç¼–ç æ–‡æœ¬
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=64,
        return_tensors='pt'
    )

    input_ids = encodings['input_ids'].to(device)
    attention_mask = encodings['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        # è·å–æ¦‚ç‡
        probabilities = torch.nn.functional.softmax(logits, dim=1)
        max_probs = torch.max(probabilities, dim=1).values

        # è½¬æ¢ä¸ºæ ‡ç­¾æ–‡æœ¬
        predicted_labels = lbl.inverse_transform(predictions.cpu().numpy())

        # è¿”å›ç»“æœ
        results = []
        for i, text in enumerate(texts):
            results.append({
                'text': text,
                'predicted_label': predicted_labels[i],
                'predicted_id': predictions[i].item(),
                'confidence': max_probs[i].item()
            })

    return results


# åˆ›å»ºä¿å­˜ç›®å½•
save_dir = 'saved_models'
os.makedirs(save_dir, exist_ok=True)

# æ—©åœæœºåˆ¶
best_accuracy = 0
patience = 3
patience_counter = 0
best_model_state = None
best_epoch = 0

# è®­ç»ƒå¾ªç¯
num_epochs = 4
print("å¼€å§‹è®­ç»ƒ...")
print(f"æ¨¡å‹å°†ä¿å­˜åˆ°ç›®å½•: {save_dir}")
print("-" * 50)

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss, train_acc = train_model(model, train_loader, optimizer, epoch)
    print(f"Epoch {epoch + 1}: è®­ç»ƒæŸå¤± = {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡ = {train_acc:.4f}")

    # è¯„ä¼°
    val_loss, val_acc, _, _ = evaluate_model(model, test_loader)
    print(f"Epoch {epoch + 1}: éªŒè¯æŸå¤± = {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡ = {val_acc:.4f}")

    # ============ ä¿å­˜æ¯ä¸ªepochçš„æ¨¡å‹ ============
    epoch_filename = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth')
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'train_accuracy': train_acc,
        'val_loss': val_loss,
        'val_accuracy': val_acc,
        'label_encoder': lbl,
        'tokenizer': tokenizer
    }, epoch_filename)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {epoch_filename}")
    # ============================================

    # æ—©åœé€»è¾‘
    if val_acc > best_accuracy + 0.001:
        best_accuracy = val_acc
        patience_counter = 0
        best_model_state = model.state_dict().copy()
        best_epoch = epoch + 1

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_filename = os.path.join(save_dir, f'best_model_epoch_{best_epoch}.pth')
        torch.save({
            'epoch': best_epoch,
            'model_state_dict': best_model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'val_accuracy': best_accuracy,
            'label_encoder': lbl,
            'tokenizer': tokenizer
        }, best_filename)
        print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_filename}")
    else:
        patience_counter += 1
        print(f"â³ æ—©åœè®¡æ•°å™¨: {patience_counter}/{patience}")

    # æ£€æŸ¥æ—©åœ
    if patience_counter >= patience:
        print("ğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶")
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"æ¢å¤æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})")
        break

    print("-" * 50)

# æœ€ç»ˆè¯„ä¼°
print("\næœ€ç»ˆè¯„ä¼°ç»“æœ:")
final_loss, final_acc, predictions, true_labels = evaluate_model(model, test_loader)
print(f"æµ‹è¯•é›†æŸå¤±: {final_loss:.4f}")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {final_acc:.4f}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
final_filename = os.path.join(save_dir, 'final_model.pth')
torch.save({
    'model_state_dict': model.state_dict(),
    'final_accuracy': final_acc,
    'final_loss': final_loss,
    'label_encoder': lbl,
    'tokenizer': tokenizer,
    'total_epochs': epoch + 1,
    'best_epoch': best_epoch
}, final_filename)
print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_filename}")

# åˆ—å‡ºä¿å­˜çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
print("\nå·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:")
model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
for model_file in sorted(model_files):
    filepath = os.path.join(save_dir, model_file)
    filesize = os.path.getsize(filepath) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    print(f"  - {model_file} ({filesize:.2f} MB)")

# # ç¤ºä¾‹ï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
# print("\n=== é¢„æµ‹ç¤ºä¾‹ ===")
# test_texts = [
#     "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
#     "å¦ä¸€ä¸ªæµ‹è¯•æ ·ä¾‹",
#     "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸå¥½"
# ]
#
# results = predict_texts(model, test_texts)
# for result in results:
#     print(f"æ–‡æœ¬: {result['text']}")
#     print(f"  é¢„æµ‹æ ‡ç­¾: {result['predicted_label']}")
#     print(f"  ç½®ä¿¡åº¦: {result['confidence']:.4f}")
#     print()


# åŠ è½½æŒ‡å®šepochæ¨¡å‹è¿›è¡Œé¢„æµ‹çš„ç¤ºä¾‹
def load_model_from_epoch(epoch_num):
    """åŠ è½½æŒ‡å®šepochçš„æ¨¡å‹"""
    model_path = os.path.join(save_dir, f'model_epoch_{epoch_num}.pth')
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)

        # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
        loaded_model = BertForSequenceClassification.from_pretrained('google-bert/bert-base-chinese',
            num_labels=17
        )
        loaded_model.to(device)
        loaded_model.load_state_dict(checkpoint['model_state_dict'])

        print(f"âœ… æˆåŠŸåŠ è½½ Epoch {epoch_num} çš„æ¨¡å‹")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_accuracy']:.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {checkpoint['train_accuracy']:.4f}")

        return loaded_model
    else:
        print(f"âŒ æ‰¾ä¸åˆ° Epoch {epoch_num} çš„æ¨¡å‹æ–‡ä»¶")
        return None


# æµ‹è¯•åŠ è½½ç‰¹å®šepochæ¨¡å‹
print("\n==================================================== åŠ è½½æ¨¡å‹æµ‹è¯• ===============================================================")
# # åŠ è½½ç¬¬1ä¸ªepochçš„æ¨¡å‹
# model_epoch1 = load_model_from_epoch(1)
# if model_epoch1:
#     results = predict_texts(model_epoch1, ["æµ‹è¯•æ–‡æœ¬"])
#     print(f"é¢„æµ‹ç»“æœ: {results[0]['predicted_label']}")

# äº¤äº’å¼é¢„æµ‹
# while True:
#     user_input = input("\nè¯·è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬ (è¾“å…¥ 'quit' é€€å‡º): ")
#     if user_input.lower() == 'quit':
#         break
#
#     if user_input.strip():
#         results = predict_texts(model, [user_input])
#         result = results[0]
#         print(f"é¢„æµ‹ç»“æœ: {result['predicted_label']}")
#         print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")

str = "ä¸€æƒ³åˆ°æ˜å¤©ï¼Œæˆ‘å°±å¯¹æ˜å¤©å……æ»¡äº†å¸Œæœ›"
results = predict_texts(model, [str])
result = results[0]
print(f"é¢„æµ‹ç»“æœ: {result['predicted_label']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")