1. BERT（Transformer架构的预训练模型）
优点：
深层语义理解
通用性强
少样本学习

缺点：
计算成本高
数据依赖
可解释性差

2. Prompt Learning（提示学习）
优点：
高效利用预训练知识
少样本/零样本能力强
灵活性强

缺点：
模板敏感
依赖预训练模型
不稳定

3. 正则规则（REGEX_RULE）
优点：
精确匹配
速度快
完全可控

缺点：
泛化能力差
维护成本高
无法理解语义

4. TF-IDF（词频-逆文档频率）
优点：
简单高效
无监督
可解释性

缺点：
忽略上下文与语义
稀疏性问题
无法处理同义词/多义词

总结
追求语义深度：选择BERT或Prompt Learning，但需承担计算成本。
追求可控性与效率：选择正则规则，但需接受有限泛化能力。
轻量级基线方案：TF-IDF可作为快速原型或简单任务首选。
组合使用：实际系统常混合使用，例如：正则处理结构化部分 + BERT处理语义部分，兼顾效率与精度。
