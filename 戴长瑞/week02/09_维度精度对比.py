import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from datetime import datetime
import warnings
import json
import os

warnings.filterwarnings('ignore')


# ==================== 1. å®éªŒé…ç½® ====================
class ArchitectureConfig:
    """ç½‘ç»œç»“æ„å®éªŒé…ç½®"""
    DATA_PATH = "../Week01/data/dataset.csv"
    MAX_LEN = 40
    BATCH_SIZE = 32
    LEARNING_RATE = 0.001
    NUM_EPOCHS = 20  # å¢åŠ epochä»¥è§‚å¯Ÿlosså˜åŒ–
    RANDOM_SEED = 42
    SAVE_DIR = "architecture_loss_experiment"

    # æµ‹è¯•çš„æ¶æ„é…ç½® (å±‚æ•°, æ¯å±‚èŠ‚ç‚¹æ•°)
    ARCHITECTURES = [
        # (å±‚æ•°, æ¯å±‚èŠ‚ç‚¹æ•°, æè¿°)
        (1, 64, "å•å±‚-64èŠ‚ç‚¹"),
        (1, 128, "å•å±‚-128èŠ‚ç‚¹"),
        (1, 256, "å•å±‚-256èŠ‚ç‚¹"),
        (2, [128, 64], "ä¸¤å±‚-128â†’64"),
        (2, [256, 128], "ä¸¤å±‚-256â†’128"),
        (2, [512, 256], "ä¸¤å±‚-512â†’256"),
        (3, [256, 128, 64], "ä¸‰å±‚-256â†’128â†’64"),
        (3, [512, 256, 128], "ä¸‰å±‚-512â†’256â†’128"),
        (4, [512, 256, 128, 64], "å››å±‚-512â†’256â†’128â†’64"),
    ]


# ==================== 2. çµæ´»çš„ç½‘ç»œæ¶æ„ ====================
class FlexibleClassifier(nn.Module):
    """æ”¯æŒä¸åŒå±‚æ•°å’ŒèŠ‚ç‚¹æ•°çš„çµæ´»åˆ†ç±»å™¨"""

    def __init__(self, input_dim, hidden_dims, output_dim):
        super(FlexibleClassifier, self).__init__()

        self.input_dim = input_dim
        if isinstance(hidden_dims, int):
            hidden_dims = [hidden_dims]  # å•å±‚æƒ…å†µ
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.num_layers = len(hidden_dims)

        # åŠ¨æ€åˆ›å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim

        # éšè—å±‚
        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))  # æ·»åŠ å°‘é‡dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

        # è®¡ç®—æ¨¡å‹å¤æ‚åº¦
        self.num_params = sum(p.numel() for p in self.parameters())
        self.trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        print(f"  åˆ›å»ºç½‘ç»œ: {input_dim} â†’ {' â†’ '.join(map(str, hidden_dims))} â†’ {output_dim}")
        print(f"  æ€»å±‚æ•°: {self.num_layers + 1} (éšè—å±‚: {self.num_layers})")
        print(f"  å‚æ•°é‡: {self.num_params:,} (å¯è®­ç»ƒ: {self.trainable_params:,})")

    def forward(self, x):
        return self.network(x)


# ==================== 3. å®éªŒæ ¸å¿ƒï¼šè®°å½•è¯¦ç»†çš„losså˜åŒ– ====================
class LossTracker:
    """è¯¦ç»†è®°å½•losså˜åŒ–çš„ç±»"""

    def __init__(self, arch_name):
        self.arch_name = arch_name
        self.epoch_losses = []  # æ¯epochçš„å¹³å‡loss
        self.batch_losses = []  # æ¯ä¸ªbatchçš„loss
        self.val_losses = []  # éªŒè¯é›†loss
        self.epoch_times = []  # æ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´
        self.gradient_norms = []  # æ¢¯åº¦èŒƒæ•°ï¼ˆè®­ç»ƒç¨³å®šæ€§ï¼‰
        self.convergence_speed = None  # æ”¶æ•›é€Ÿåº¦
        self.final_loss = None  # æœ€ç»ˆloss
        self.batch_loss_std = 0.0       # lossçš„æ ‡å‡†å·®

    def add_epoch_result(self, epoch, train_loss, val_loss, batch_losses, epoch_time):
        """è®°å½•ä¸€ä¸ªepochçš„ç»“æœ"""
        self.epoch_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.batch_losses.extend(batch_losses)
        self.epoch_times.append(epoch_time)

        # è®¡ç®—æ”¶æ•›é€Ÿåº¦ï¼ˆlossä¸‹é™åˆ°åˆå§‹å€¼10%æ‰€éœ€çš„epochæ•°ï¼‰
        if len(self.epoch_losses) >= 2:
            if self.epoch_losses[0] > 0:
                current_ratio = train_loss / self.epoch_losses[0]
                if current_ratio < 0.1 and self.convergence_speed is None:
                    self.convergence_speed = epoch + 1
        # è®¡ç®—batch lossçš„æ ‡å‡†å·®
        if len(batch_losses) > 0:
            self.batch_loss_std = np.std(batch_losses)
    def get_summary(self):
        """è·å–losså˜åŒ–çš„ç»Ÿè®¡æ‘˜è¦"""
        if not self.epoch_losses:
            return {}

        # è®¡ç®—losså˜åŒ–ç»Ÿè®¡
        initial_loss = self.epoch_losses[0] if self.epoch_losses else 0
        final_loss = self.epoch_losses[-1] if self.epoch_losses else 0
        self.final_loss = final_loss

        # è®¡ç®—lossä¸‹é™çš„ç»Ÿè®¡
        loss_reduction = initial_loss - final_loss
        loss_reduction_pct = (loss_reduction / initial_loss * 100) if initial_loss > 0 else 0

        # è®¡ç®—è®­ç»ƒç¨³å®šæ€§ï¼ˆbatch lossçš„æ–¹å·®ï¼‰
        if len(self.batch_losses) > 10:
            recent_batch_losses = self.batch_losses[-100:]  # æœ€è¿‘çš„100ä¸ªbatch
            batch_loss_std = np.std(recent_batch_losses)
        else:
            batch_loss_std = 0

        return {
            'architecture': self.arch_name,
            'initial_loss': float(initial_loss),
            'final_loss': float(final_loss),
            'loss_reduction': float(loss_reduction),
            'loss_reduction_pct': float(loss_reduction_pct),
            'convergence_speed': self.convergence_speed if self.convergence_speed else ArchitectureConfig.NUM_EPOCHS,
            'avg_epoch_time': float(np.mean(self.epoch_times)) if self.epoch_times else 0,
             'batch_loss_std': float(self.batch_loss_std) if self.batch_loss_std is not None else 0.0,
            'num_epochs_tracked': len(self.epoch_losses)
        }


# ==================== 4. è®­ç»ƒå‡½æ•°ï¼ˆè®°å½•è¯¦ç»†losså˜åŒ–ï¼‰ ====================
def train_architecture(model, train_loader, val_loader, arch_name):
    """è®­ç»ƒä¸€ä¸ªç‰¹å®šæ¶æ„çš„æ¨¡å‹ï¼Œè¯¦ç»†è®°å½•losså˜åŒ–"""
    criterion = nn.CrossEntropyLoss()
    # optimizer = optim.SGD(model.parameters(), lr=ArchitectureConfig.LEARNING_RATE)    #æ‰‹åŠ¨
    optimizer = optim.Adam(model.parameters(), lr=ArchitectureConfig.LEARNING_RATE)     #è‡ªé€‚åº”

    # åˆ›å»ºlossè·Ÿè¸ªå™¨
    tracker = LossTracker(arch_name)

    print(f"\n  å¼€å§‹è®­ç»ƒ {arch_name}...")

    for epoch in range(ArchitectureConfig.NUM_EPOCHS):
        start_time = datetime.now()

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0
        batch_losses_this_epoch = []

        for batch_idx, (inputs, targets) in enumerate(train_loader):
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()

            # è®°å½•æ¢¯åº¦èŒƒæ•°ï¼ˆè®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡ï¼‰
            total_norm = 0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            tracker.gradient_norms.append(total_norm)

            optimizer.step()

            epoch_loss += loss.item()
            batch_losses_this_epoch.append(loss.item())

            # æ¯25%çš„batchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if (batch_idx + 1) % max(1, len(train_loader) // 4) == 0:
                progress = (batch_idx + 1) / len(train_loader) * 100
                print(f"    Epoch {epoch + 1:2d} | è¿›åº¦: {progress:5.1f}% | Batch Loss: {loss.item():.4f}")

        avg_train_loss = epoch_loss / len(train_loader)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * correct / total
        epoch_time = (datetime.now() - start_time).total_seconds()

        # è®°å½•ç»“æœ
        tracker.add_epoch_result(epoch, avg_train_loss, avg_val_loss,
                                 batch_losses_this_epoch, epoch_time)

        # æ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡è¯¦ç»†ç»“æœ
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"    Epoch {epoch + 1:3d}/{ArchitectureConfig.NUM_EPOCHS} | "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {avg_val_loss:.4f} | "
                  f"Val Acc: {val_accuracy:.2f}% | "
                  f"Time: {epoch_time:.1f}s")

    return tracker, val_accuracy


# ==================== 5. è¿è¡Œæ¶æ„å®éªŒ ====================
def run_architecture_experiment():
    """è¿è¡Œä¸åŒæ¶æ„çš„å®éªŒ"""
    print("=" * 70)
    print(" ç½‘ç»œæ¶æ„å®éªŒ: å±‚æ•° vs èŠ‚ç‚¹æ•° vs Losså˜åŒ–")
    print("=" * 70)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(ArchitectureConfig.RANDOM_SEED)
    np.random.seed(ArchitectureConfig.RANDOM_SEED)

    # åŠ è½½æ•°æ®
    print("\n åŠ è½½æ•°æ®...")
    dataset = pd.read_csv(ArchitectureConfig.DATA_PATH, sep="\t", header=None)
    texts = dataset[0].tolist()
    labels = dataset[1].tolist()

    # é¢„å¤„ç†
    label_to_idx = {label: i for i, label in enumerate(sorted(set(labels)))}
    label_indices = [label_to_idx[label] for label in labels]

    char_to_idx = {'<pad>': 0}
    for text in texts:
        for char in text:
            if char not in char_to_idx:
                char_to_idx[char] = len(char_to_idx)

    vocab_size = len(char_to_idx)
    num_classes = len(label_to_idx)

    print(f" æ•°æ®ç»Ÿè®¡:")
    print(f"  â€¢ æ ·æœ¬æ•°: {len(texts)}")
    print(f"  â€¢ è¯æ±‡è¡¨: {vocab_size}")
    print(f"  â€¢ ç±»åˆ«æ•°: {num_classes}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {ArchitectureConfig.NUM_EPOCHS}")
    print(f"  â€¢ æµ‹è¯•æ¶æ„æ•°: {len(ArchitectureConfig.ARCHITECTURES)}")

    # åˆ›å»ºæ•°æ®é›†
    class TextDataset(Dataset):
        def __init__(self, texts, labels, char_to_idx, max_len, vocab_size):
            self.labels = torch.tensor(labels, dtype=torch.long)
            self.features = self._create_features(texts, char_to_idx, max_len, vocab_size)

        def _create_features(self, texts, char_to_idx, max_len, vocab_size):
            features = []
            for text in texts:
                encoded = [char_to_idx.get(char, 0) for char in text[:max_len]]
                encoded += [0] * (max_len - len(encoded))
                bow = torch.zeros(vocab_size)
                for idx in encoded:
                    if idx != 0:
                        bow[idx] += 1
                features.append(bow)
            return torch.stack(features)

        def __len__(self):
            return len(self.labels)

        def __getitem__(self, idx):
            return self.features[idx], self.labels[idx]

    dataset = TextDataset(texts, label_indices, char_to_idx,
                          ArchitectureConfig.MAX_LEN, vocab_size)

    # åˆ’åˆ†æ•°æ®é›†
    train_size = int(0.7 * len(dataset))  # 70%è®­ç»ƒ
    val_size = int(0.15 * len(dataset))  # 15%éªŒè¯
    test_size = len(dataset) - train_size - val_size

    train_dataset, temp_dataset = random_split(dataset, [train_size, len(dataset) - train_size])
    val_dataset, test_dataset = random_split(temp_dataset, [val_size, test_size])

    train_loader = DataLoader(train_dataset, batch_size=ArchitectureConfig.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=ArchitectureConfig.BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=ArchitectureConfig.BATCH_SIZE, shuffle=False)

    print(f"\n æ•°æ®é›†åˆ’åˆ†:")
    print(f"  â€¢ è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"  â€¢ éªŒè¯é›†: {val_size} æ ·æœ¬")
    print(f"  â€¢ æµ‹è¯•é›†: {test_size} æ ·æœ¬")

    # è¿è¡Œæ‰€æœ‰æ¶æ„å®éªŒ
    print(f"\n{'=' * 70}")
    print(" å¼€å§‹æ¶æ„å®éªŒ...")
    print(f"{'=' * 70}")

    results = []
    trackers = []

    for arch_config in ArchitectureConfig.ARCHITECTURES:
        num_layers, hidden_dims, description = arch_config

        print(f"\n æµ‹è¯•æ¶æ„: {description}")
        print(f"{'-' * 60}")

        # åˆ›å»ºæ¨¡å‹
        model = FlexibleClassifier(vocab_size, hidden_dims, num_classes)

        # è®­ç»ƒæ¨¡å‹
        tracker, val_accuracy = train_architecture(model, train_loader, val_loader, description)

        # æ”¶é›†ç»“æœ
        summary = tracker.get_summary()
        summary.update({
            'num_layers': num_layers,
            'hidden_dims': hidden_dims if isinstance(hidden_dims, list) else [hidden_dims],
            'num_params': model.num_params,
            'final_val_accuracy': val_accuracy,
            'description': description
        })

        results.append(summary)
        trackers.append(tracker)

        print(f" å®Œæˆ! æœ€ç»ˆè®­ç»ƒLoss: {summary['final_loss']:.4f}, "
              f"éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%")

    return results, trackers, vocab_size, num_classes


# ==================== 6. å¯è§†åŒ–Losså˜åŒ–åˆ†æ ====================
def visualize_loss_analysis(results, trackers):
    """å¯è§†åŒ–losså˜åŒ–åˆ†æ"""
    print(f"\n{'=' * 70}")
    print(" Losså˜åŒ–å¯è§†åŒ–åˆ†æ")
    print(f"{'=' * 70}")

    os.makedirs(ArchitectureConfig.SAVE_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºç»“æœDataFrame
    df = pd.DataFrame(results)

    # 1. ç»¼åˆå¯¹æ¯”å›¾
    fig = plt.figure(figsize=(20, 12))

    # å›¾1: å„æ¶æ„çš„Lossä¸‹é™æ›²çº¿å¯¹æ¯”
    ax1 = plt.subplot(2, 3, 1)
    colors = plt.cm.tab20(np.linspace(0, 1, len(trackers)))

    for idx, tracker in enumerate(trackers):
        epochs = range(1, len(tracker.epoch_losses) + 1)
        ax1.plot(epochs, tracker.epoch_losses,
                 color=colors[idx], linewidth=2.5, alpha=0.8,
                 label=tracker.arch_name)

    ax1.set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=11, fontweight='bold')
    ax1.set_ylabel('è®­ç»ƒLoss', fontsize=11, fontweight='bold')
    ax1.set_title('ä¸åŒæ¶æ„çš„è®­ç»ƒLossä¸‹é™æ›²çº¿', fontsize=13, fontweight='bold', pad=15)
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')  # å¯¹æ•°åæ ‡æ›´å¥½è§‚å¯Ÿä¸‹é™è¶‹åŠ¿

    # å›¾2: åˆå§‹Loss vs æœ€ç»ˆLoss
    ax2 = plt.subplot(2, 3, 2)
    scatter = ax2.scatter(df['initial_loss'], df['final_loss'],
                          c=df['num_layers'], s=df['num_params'] / 1000,
                          cmap='viridis', alpha=0.7, edgecolors='black')

    # æ·»åŠ è¿æ¥çº¿æ˜¾ç¤ºä¸‹é™
    for idx, row in df.iterrows():
        ax2.plot([row['initial_loss'], row['final_loss']],
                 [row['initial_loss'], row['final_loss']],
                 'k--', alpha=0.2, linewidth=0.5)
        ax2.annotate('', xy=(row['final_loss'], row['final_loss']),
                     xytext=(row['initial_loss'], row['initial_loss']),
                     arrowprops=dict(arrowstyle='->', color='gray', alpha=0.5))

    ax2.set_xlabel('åˆå§‹Loss', fontsize=11, fontweight='bold')
    ax2.set_ylabel('æœ€ç»ˆLoss', fontsize=11, fontweight='bold')
    ax2.set_title('åˆå§‹Loss vs æœ€ç»ˆLoss (å¤§å°=å‚æ•°é‡/1000)', fontsize=13, fontweight='bold', pad=15)
    ax2.grid(True, alpha=0.3)

    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(scatter, ax=ax2, label='å±‚æ•°')

    # å›¾3: æ”¶æ•›é€Ÿåº¦åˆ†æ
    ax3 = plt.subplot(2, 3, 3)
    bars = ax3.bar(range(len(df)), df['loss_reduction_pct'],
                   color=plt.cm.coolwarm(df['convergence_speed'] / max(df['convergence_speed'])))

    ax3.set_xlabel('ç½‘ç»œæ¶æ„', fontsize=11, fontweight='bold')
    ax3.set_ylabel('Lossä¸‹é™ç™¾åˆ†æ¯” (%)', fontsize=11, fontweight='bold')
    ax3.set_title('Lossä¸‹é™æ•ˆæœä¸æ”¶æ•›é€Ÿåº¦', fontsize=13, fontweight='bold', pad=15)
    ax3.set_xticks(range(len(df)))
    ax3.set_xticklabels([f"L{d['num_layers']}-N{sum(d['hidden_dims']) // len(d['hidden_dims'])}"
                         for d in results], rotation=45, ha='right')

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ”¶æ•›é€Ÿåº¦
    for i, (bar, speed) in enumerate(zip(bars, df['convergence_speed'])):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width() / 2., height + 1,
                 f'E{speed}', ha='center', va='bottom', fontsize=9, fontweight='bold')

    # å›¾4: Batch Lossæ³¢åŠ¨åˆ†æï¼ˆè®­ç»ƒç¨³å®šæ€§ï¼‰
    ax4 = plt.subplot(2, 3, 4)

    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ¶æ„å±•ç¤ºbatch lossæ³¢åŠ¨
    sample_indices = [0, 3, 6, 8]  # é€‰æ‹©å•å±‚ã€ä¸¤å±‚ã€ä¸‰å±‚ã€å››å±‚å„ä¸€ä¸ª
    sample_colors = ['red', 'blue', 'green', 'purple']

    for idx, color in zip(sample_indices, sample_colors):
        tracker = trackers[idx]
        if len(tracker.batch_losses) > 100:
            # å–æœ€å100ä¸ªbatchå±•ç¤º
            batch_indices = range(len(tracker.batch_losses) - 100, len(tracker.batch_losses))
            batch_losses = tracker.batch_losses[-100:]

            # ä½¿ç”¨ç§»åŠ¨å¹³å‡å¹³æ»‘
            window = 5
            if len(batch_losses) > window:
                smoothed = np.convolve(batch_losses, np.ones(window) / window, mode='valid')
                ax4.plot(range(len(smoothed)), smoothed,
                         color=color, linewidth=1.5, alpha=0.7,
                         # label=f"{tracker.arch_name} (std={tracker.batch_loss_std:.4f})"
                         label=f"{tracker.arch_name} (std={tracker.batch_loss_std:.4f})")

    ax4.set_xlabel('Batchåºå· (æœ€è¿‘100ä¸ª)', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Batch Loss', fontsize=11, fontweight='bold')
    ax4.set_title('Batch Lossæ³¢åŠ¨åˆ†æ (è®­ç»ƒç¨³å®šæ€§)', fontsize=13, fontweight='bold', pad=15)
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3)

    # å›¾5: å±‚æ•° vs Lossä¸‹é™æ•ˆæœ
    ax5 = plt.subplot(2, 3, 5)

    # æŒ‰å±‚æ•°åˆ†ç»„
    layers_grouped = df.groupby('num_layers')
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

    for (layer_num, group), color in zip(layers_grouped, colors):
        ax5.scatter(group['num_params'], group['final_loss'],
                    s=150, color=color, alpha=0.7, edgecolors='black',
                    label=f'{layer_num}å±‚ç½‘ç»œ')

    ax5.set_xlabel('æ¨¡å‹å‚æ•°é‡', fontsize=11, fontweight='bold')
    ax5.set_ylabel('æœ€ç»ˆè®­ç»ƒLoss', fontsize=11, fontweight='bold')
    ax5.set_title('å±‚æ•°ä¸å‚æ•°é‡å¯¹Lossçš„å½±å“', fontsize=13, fontweight='bold', pad=15)
    ax5.legend(fontsize=9)
    ax5.grid(True, alpha=0.3)
    ax5.set_xscale('log')

    # å›¾6: è®­ç»ƒæ—¶é—´åˆ†æ
    ax6 = plt.subplot(2, 3, 6)

    x_pos = np.arange(len(df))
    bars1 = ax6.bar(x_pos - 0.2, df['avg_epoch_time'], 0.4,
                    label='æ¯è½®æ—¶é—´', color='skyblue')
    bars2 = ax6.bar(x_pos + 0.2, df['convergence_speed'] * df['avg_epoch_time'], 0.4,
                    label='æ”¶æ•›æ€»æ—¶é—´', color='lightcoral')

    ax6.set_xlabel('ç½‘ç»œæ¶æ„', fontsize=11, fontweight='bold')
    ax6.set_ylabel('æ—¶é—´ (ç§’)', fontsize=11, fontweight='bold')
    ax6.set_title('è®­ç»ƒæ—¶é—´æ•ˆç‡åˆ†æ', fontsize=13, fontweight='bold', pad=15)
    ax6.set_xticks(x_pos)
    ax6.set_xticklabels([d['description'][:15] for d in results], rotation=45, ha='right')
    ax6.legend(fontsize=9)
    ax6.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(f'{ArchitectureConfig.SAVE_DIR}/architecture_loss_analysis_{timestamp}.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # 2. è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
    print(f"\n æ¶æ„æ€§èƒ½å¯¹æ¯”è¡¨")
    print("-" * 90)
    print(f"{'æ¶æ„æè¿°':<20} {'å±‚æ•°':<6} {'å‚æ•°é‡':<12} {'åˆå§‹Loss':<10} {'æœ€ç»ˆLoss':<10} "
          f"{'ä¸‹é™%':<8} {'æ”¶æ•›é€Ÿåº¦':<10} {'éªŒè¯å‡†ç¡®ç‡':<12}")
    print("-" * 90)

    df_sorted = df.sort_values('final_loss')
    for _, row in df_sorted.iterrows():
        print(f"{row['description'][:18]:<20} {row['num_layers']:<6} "
              f"{row['num_params']:<12,} {row['initial_loss']:<10.4f} "
              f"{row['final_loss']:<10.4f} {row['loss_reduction_pct']:<8.1f}% "
              f"Epoch {row['convergence_speed']:<8} {row['final_val_accuracy']:<11.2f}%")

    return df, trackers, timestamp


# ==================== 7. ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š ====================
def generate_analysis_report(df, trackers, vocab_size, num_classes, timestamp):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    print(f"\n{'=' * 70}")
    print(" ç½‘ç»œæ¶æ„å¯¹Losså˜åŒ–å½±å“åˆ†ææŠ¥å‘Š")
    print(f"{'=' * 70}")

    # æ‰¾åˆ°æœ€ä½³æ¶æ„ï¼ˆç»¼åˆè€ƒè™‘Losså’Œå‡†ç¡®ç‡ï¼‰
    df['score'] = (100 - df['final_loss'] * 10) + (df['final_val_accuracy'] / 2)
    best_idx = df['score'].idxmax()
    best_arch = df.loc[best_idx]

    print(f"\n å®éªŒé…ç½®:")
    print(f"  â€¢ è¾“å…¥ç»´åº¦: {vocab_size}")
    print(f"  â€¢ è¾“å‡ºç»´åº¦: {num_classes}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {ArchitectureConfig.NUM_EPOCHS}")
    print(f"  â€¢ æµ‹è¯•æ¶æ„æ•°: {len(df)}")

    print(f"\n æœ€ä½³æ€§èƒ½æ¶æ„:")
    print(f"  â€¢ æ¶æ„: {best_arch['description']}")
    print(f"  â€¢ å±‚æ•°: {best_arch['num_layers']}")
    print(f"  â€¢ æœ€ç»ˆLoss: {best_arch['final_loss']:.4f}")
    print(f"  â€¢ éªŒè¯å‡†ç¡®ç‡: {best_arch['final_val_accuracy']:.2f}%")
    print(f"  â€¢ æ”¶æ•›é€Ÿåº¦: {best_arch['convergence_speed']}ä¸ªepoch")

    print(f"\n å…³é”®å‘ç°:")

    # åˆ†æ1: å±‚æ•°å¯¹Lossçš„å½±å“
    layer_analysis = df.groupby('num_layers').agg({
        'final_loss': ['mean', 'min', 'max'],
        'convergence_speed': 'mean',
        'final_val_accuracy': 'mean'
    }).round(4)

    print(f"1. å±‚æ•°å½±å“åˆ†æ:")
    for layers, stats in layer_analysis.iterrows():
        print(f"   {layers}å±‚ç½‘ç»œ: å¹³å‡Loss={stats[('final_loss', 'mean')]:.4f}, "
              f"å¹³å‡å‡†ç¡®ç‡={stats[('final_val_accuracy', 'mean')]:.2f}%, "
              f"æ”¶æ•›é€Ÿåº¦={stats[('convergence_speed', 'mean')]:.1f} epoch")

    # åˆ†æ2: å‚æ•°é‡ä¸Lossçš„å…³ç³»
    corr_params_loss = df['num_params'].corr(df['final_loss'])
    print(f"\n2. å‚æ•°é‡ä¸Lossç›¸å…³æ€§: {corr_params_loss:.4f}")
    if corr_params_loss > 0.3:
        print("   â†’ å‚æ•°é‡å¢åŠ å¯èƒ½å¯¼è‡´Lossä¸Šå‡ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")
    elif corr_params_loss < -0.3:
        print("   â†’ å‚æ•°é‡å¢åŠ æœ‰åŠ©äºé™ä½Loss")
    else:
        print("   â†’ å‚æ•°é‡ä¸Losså…³ç³»ä¸æ˜æ˜¾")

    # åˆ†æ3: æ”¶æ•›é€Ÿåº¦åˆ†æ
    fastest_idx = df['convergence_speed'].idxmin()
    fastest_arch = df.loc[fastest_idx]
    print(f"\n3. æœ€å¿«æ”¶æ•›æ¶æ„: {fastest_arch['description']}")
    print(f"   ä»…éœ€{fastest_arch['convergence_speed']}ä¸ªepochè¾¾åˆ°ç¨³å®š")
    print(f"   æœ€ç»ˆLoss: {fastest_arch['final_loss']:.4f}")

    # åˆ†æ4: è®­ç»ƒç¨³å®šæ€§åˆ†æ
    stable_idx = df['batch_loss_std'].idxmin()
    stable_arch = df.loc[stable_idx]
    print(f"\n4. æœ€ç¨³å®šè®­ç»ƒæ¶æ„: {stable_arch['description']}")
    print(f"   Batch Lossæ ‡å‡†å·®: {stable_arch['batch_loss_std']:.4f}")
    print(f"   ï¼ˆæ³¢åŠ¨è¶Šå°ï¼Œè®­ç»ƒè¶Šç¨³å®šï¼‰")

    print(f"\nğŸ’¡ å®è·µå»ºè®®:")

    # åŸºäºå®éªŒç»“æœçš„å»ºè®®
    if best_arch['num_layers'] == 1:
        print("1. å•å±‚ç½‘ç»œæ•ˆæœæœ€å¥½ï¼Œè¯´æ˜ä»»åŠ¡ç›¸å¯¹ç®€å•")
        print("2. ä¸éœ€è¦å¤æ‚ç½‘ç»œï¼Œå¯å‡å°‘è®¡ç®—èµ„æº")
    elif best_arch['num_layers'] == 2:
        print("1. åŒå±‚ç½‘ç»œæ˜¯æœ€ä½³å¹³è¡¡ç‚¹")
        print("2. æ—¢æœ‰è¶³å¤Ÿè¡¨è¾¾èƒ½åŠ›ï¼Œåˆä¸ä¼šè¿‡æ‹Ÿåˆ")
    else:
        print("1. å¤šå±‚ç½‘ç»œæ•ˆæœæœ€ä½³ï¼Œä½†éœ€è¦è¶³å¤Ÿæ•°æ®")
        print("2. è€ƒè™‘æ·»åŠ æ›´å¤šæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ")

    # æ•ˆç‡å»ºè®®
    efficient_idx = (df['final_val_accuracy'] / df['num_params'] * 1e6).idxmax()
    efficient_arch = df.loc[efficient_idx]

    print(f"\n2. æ•ˆç‡æœ€ä½³æ¶æ„: {efficient_arch['description']}")
    print(f"   æ¯ç™¾ä¸‡å‚æ•°å‡†ç¡®ç‡: {efficient_arch['final_val_accuracy'] / efficient_arch['num_params'] * 1e6:.4f}")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    df.to_csv(f'{ArchitectureConfig.SAVE_DIR}/architecture_results_{timestamp}.csv',
              index=False, encoding='utf-8-sig')

    # ä¿å­˜è¯¦ç»†lossæ›²çº¿æ•°æ®
    loss_curves = {}
    for tracker in trackers:
        loss_curves[tracker.arch_name] = {
            'epoch_losses': tracker.epoch_losses,
            'val_losses': tracker.val_losses,
            'batch_loss_std': tracker.batch_loss_std
        }

    with open(f'{ArchitectureConfig.SAVE_DIR}/loss_curves_{timestamp}.json', 'w') as f:
        json.dump(loss_curves, f, indent=2)

    print(f"\n ç»“æœå·²ä¿å­˜è‡³: {ArchitectureConfig.SAVE_DIR}/")
    print(f"   å›¾è¡¨: architecture_loss_analysis_{timestamp}.png")
    print(f"   æ•°æ®: architecture_results_{timestamp}.csv")
    print(f"   Lossæ›²çº¿: loss_curves_{timestamp}.json")

    return best_arch


# ==================== 8. ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print(" ç½‘ç»œæ¶æ„å®éªŒ: å±‚æ•° vs èŠ‚ç‚¹æ•° vs Losså˜åŒ–")
    print("=" * 70)

    try:
        # è¿è¡Œå®éªŒ
        results, trackers, vocab_size, num_classes = run_architecture_experiment()

        # å¯è§†åŒ–åˆ†æ
        df, trackers, timestamp = visualize_loss_analysis(results, trackers)

        # ç”ŸæˆæŠ¥å‘Š
        best_arch = generate_analysis_report(df, trackers, vocab_size, num_classes, timestamp)

        print(f"\n{'=' * 70}")
        print(" å®éªŒå®Œæˆ!")
        print(f" æœ€ä½³æ¶æ„: {best_arch['description']}")
        print(f" æœ€ç»ˆLoss: {best_arch['final_loss']:.4f}")
        print(f" æ”¶æ•›é€Ÿåº¦: {best_arch['convergence_speed']}ä¸ªepoch")
        print("=" * 70)

        # ç”Ÿæˆæ¨èé…ç½®ä»£ç 
        print(f"\n æ¨èé…ç½®ä»£ç :")
        print("-" * 40)

        hidden_dims = best_arch['hidden_dims']
        if len(hidden_dims) == 1:
            layers_code = f"hidden_dim = {hidden_dims[0]}"
        else:
            layers_code = f"hidden_dims = {hidden_dims}"

        print(f"""```python
# åŸºäºæ¶æ„å®éªŒçš„æœ€ä½³é…ç½®
vocab_size = {vocab_size}
{layers_code}
output_dim = {num_classes}

# åˆ›å»ºæ¨¡å‹
model = FlexibleClassifier(vocab_size, hidden_dims, output_dim)
print(f"æ¶æ„: {{model.num_layers}}å±‚, {{'â†’'.join(map(str, model.hidden_dims))}}")
print(f"å‚æ•°é‡: {{model.num_params:,}}")
print(f"é¢„æœŸæœ€ç»ˆLoss: {best_arch['final_loss']:.4f}")
print(f"é¢„æœŸéªŒè¯å‡†ç¡®ç‡: {best_arch['final_val_accuracy']:.2f}%")
```""")

    except Exception as e:
        print(f"\n å®éªŒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()